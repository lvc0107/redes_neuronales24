{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicio 1**\n",
        "\n",
        "Genere un conjunto de entrenamiento compuesto por $M=\\sum_c m_c$ puntos en $\\mathbb{R}^{n_e}$ distribuidos en $n_s$ nubes de $m_c$ puntos.\n",
        "\n",
        "Para generar las nubes:\n",
        "\n",
        "* genere aleatoriamente $n_s$ puntos en $\\mathbb{R}^{n_e}$ a los que llamaremos centros, sorteando los valores de las coordenadas a partir de una distribución normal, y\n",
        "\n",
        "* para cada centro $c$, genere $m_c$ puntos aleatorios alrededor del mismo, sumando sus coordenadas a números aleatorios generados con una Gaussiana de varianza $\\sigma^2$.\n",
        "\n",
        "Las $n_e$ coordenadas del $m$-ésimo punto constituirán el vector de entrada del $m$-ésimo ejemplo.\n",
        "La nube a la que pertenece el $m$-ésimo punto determinará el vector de salida del $m$-ésimo ejemplo.\n",
        "Más precisamente, si el $m$-ésimo punto pertenence a la $c$-ésima nube, el vector de salida será el vector canónico $(0,0,...,1,...,0)$ de $n_s$ componentes con un único 1 en la $c$-esima posición.\n",
        "\n",
        "Concretamente\n",
        "\n",
        "1. Genere un conjunto de 8 puntos en $\\mathbb{R}^{n_e}$ con $n_e=2$, divididos en 3 nubes con $m_1=3$ en la primera nube, $m_2=2$ puntos en la segunda nube y $m_3=3$ puntos en la tercera nube. Utilice $\\sigma=0.1$ para indicar la dispersión de los puntos alrededor de cada nube.\n",
        "\n",
        "2. Grafique las nubes de puntos, utilizando un color distinto para cada una de ellas.\n",
        "\n",
        "## **Ejercicio 2**\n",
        "\n",
        "1. Implemente un **perceptrón multicapa** con $n_e=2$ neuronas de entrada, una capa oculta de $n_o=2$ neuronas, y una capa de salida de $n_s=3$ neuronas. Recuerde, además, agregar las neuroas auxiliares que se utiliza para imitar los umbrales de activación. Utilice funciones de activación **sigmoideas**.\n",
        "\n",
        "2. Entrenelo sobre el conjunto de ejemplos generado en el Ejercicio 1. Para entrenarlo, utilice una tasa $\\eta=0.02$ y alrededor de 10.000 de épocas o más, según considere necesario.\n",
        "\n",
        "3. Grafique el error $E$ en función del número de épocas de entrenamiento.\n",
        "\n",
        "4. Luego, grafique nuevamente los puntos del Ejercicio 1, pintando el relleno de los mismos con los colores correspondiente a cada nube, y el borde de los mismos con el color correspondiente a la predicción obtenida con el **perceptrón multicapa**. Coinciden las predicciones con los colores originales?\n",
        "\n",
        "5. Repita los experimentos con funciones de activación **ReLUs**. Que ocurre?\n",
        "\n",
        "## **Ejercicio 3: la compuerta XOR**\n",
        "\n",
        "1. Fabrique un dataset con el siguiente conjunto de 4 ejemplos:\n",
        "\n",
        "    * $e_1 = (0,0)$, $s_1=(1,0)$\n",
        "    * $e_2 = (0,1)$, $s_2=(0,1)$\n",
        "    * $e_3 = (1,0)$, $s_3=(0,1)$\n",
        "    * $e_4 = (1,1)$, $s_4=(1,0)$\n",
        "    \n",
        "  corresponde a la compuerta XOR.\n",
        "\n",
        "2. Es el **perceptrón multicapa** capáz de aprender la compuerta XOR? Para responder esta pregunta, genere un **perceptrón multicapa** con $n_e=2$ neuronas de entrada, $n_o=2$ neuronas ocultas y $n_s=2$ neuronas de salida, y entrénelo utilizando el conjunto de ejemplos de la compuerta XOR.\n",
        "\n",
        "3. Como se compara el **perceptrón multicapa** con el **perceptrón monocapa** sobre la compuerta XOR? Para responder esta otra pregunta, genere otro perceptrón \"multicapa\", pero esta vez utilizando solo dos capas, una de entrada con $n_e=2$ neuronas y una de salida con $n_s=2$ neuronas (de manera tal que en realidad es un perceptron monocapa), y repita el experimento anterior con los ejemplos de la compuerta XOR.\n",
        "\n",
        "4. Repita los experimentos con funciones de activación **ReLUs**. Que ocurre?"
      ],
      "metadata": {
        "id": "-XeWtDvx6C8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptrón multicapa\n",
        "\n",
        "Consideraremos un perceptrón multicapa, con capas enumeradas por $l=0,1,...,L$.\n",
        "Denotemos por $x^l_i$ el estado de la $i$-ésima neurona en la capa $l$.\n",
        "Diremos que la red posee $n^l$ neuronas $i=1,...,n^l$ en la $l$-ésima capa.\n",
        "En particular, $x^0$ denota el vector de estados de la capa de entrada y $x^L$ el vector de estados de la capa de salida.\n",
        "Se tiene que\n",
        "\\begin{equation}\n",
        "x^l_i\n",
        "=\n",
        "g(h^l_i)\n",
        "\\;\\;\\;\\;\\;\\;\\;\\; (1)\n",
        "\\end{equation}\n",
        "donde $g:\\mathbb{R}\\to \\mathbb{R}$ es una función de activación, por ejemplo una sigmoide $g(h)=1/(1+e^{-h})$, y\n",
        "\\begin{equation}\n",
        "h^{l}_i\n",
        "=\n",
        "\\sum_j w^{l}_{ij} x^{l-1}_j\n",
        "\\;\\;\\;\\;\\;\\;\\;\\; (2)\n",
        "\\end{equation}\n",
        "es el campo local sufrido por la $i$-ésima neurona en la $l$-ésima capa .\n",
        "Además, $w^l_{ij}$ denota la intensidad de la sinapsis que conecta la $j$-ésima neurona en la $(l-1)$-ésima capa con la $i$-ésima neurona en la $l$-ésima capa.\n",
        "Notar, la red depende de las matrices de pesos sinápticos $w^1,w^2,...,w^{L}$.\n",
        "\n",
        "## Umbrales de activación\n",
        "\n",
        "En cada una de las capas $l=0,1,...,L-1$, se agrega una neurona extra $i=n^l+1$ con un estado fijo $x^l_{n^l+1}=-1$.\n",
        "De esta manera, una nueva sinapsis $u^{l}_i:=w^{l}_{i,n^{l-1}+1}$ hace las veces de umbral de activación de la $i$-ésima neurona en la $l$-ésima capa, ya que\n",
        "\\begin{equation}\n",
        "h^{l+1}_i\n",
        "=\n",
        "w^{l+1}_{i,n^{l}+1} x^{l}_{n^{l}+1}\n",
        "+\n",
        "\\sum_{j=1}^{n^l} w^{l+1}_{ij} x^{l}_j\n",
        "=\n",
        "-\n",
        "u^{l+1}_i\n",
        "+\n",
        "\\sum_{j=1}^{n^l} w^{l+1}_{ij} x^l_j\n",
        "\\;\\;\\;\\;\\;\\;\\;\\; (3)\n",
        "\\end{equation}\n",
        "\n",
        "## Conjunto de entrenamiento\n",
        "\n",
        "Los datos de entrenamiento consisten en un conjunto de pares $\\{(e^m,s^m):m=1,...,M\\}$ donde $e^m\\in \\mathbb{R}^{n_0}$ y $s^m\\in \\mathbb{R}^{n_L}$ son vectores que representan el $m$-ésimo par de entrada-salida o *ejemplo* que debe aprender la red.\n",
        "\n",
        "## Función costo: el Error Cuadrático\n",
        "\n",
        "Si pensamos que la salida de la red es una función de la entrada, i.e. que $x^L(x^0)$, podemos evaluar el error que comete la red sobre el conjunto de entramiento utilizando el *error cuadrático*\n",
        "$$\n",
        "E\n",
        "=\n",
        "\\sum_{m=1}^M F^m\n",
        "$$\n",
        "como *función costo*, donde\n",
        "$$\n",
        "F^m\n",
        "=\n",
        "\\frac{1}{2}\n",
        "\\sum_{i=1}^{n^L}\n",
        "(x^L_i(x^0=e^m) - s^m_i)^2\n",
        "$$\n",
        "es el error cuadrático que comete la red sobre el $m$-ésimo ejemplo.\n",
        "\n",
        "## Entrenamiento: descenso por el gradiente\n",
        "\n",
        "Entrenar la red consisten en encontrar valores de los pesos sinápticos $w^l_{ij}$ que minimicen el error $E$.\n",
        "Para ello, expresamos el error en función de dichos pesos y calculamos las componentes de su gradiente\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial w^l_{ij}}\n",
        "=\n",
        "\\sum_m\n",
        "\\frac{\\partial F^m}{\\partial w^l_{ij}}\n",
        "$$\n",
        "De esta manera, podemos utilizar el algoritmo de descenso por el gradiente para actualizar los pesos hasta que el error alcance un mínimo global.\n",
        "Más precisamente, partiendo de valores aleatorios\n",
        "$(w^l_{ij})^0$ para los pesos sinápticos, actualizamos iterativamente a los mismos con la siguiente regla\n",
        "\\begin{equation}\n",
        "(w^l_{ij})^{t+1} = (w^l_{ij})^t-\\eta \\frac{\\partial F^m}{\\partial w^l_{ij}}((w^l_{ij})^t)\n",
        "\\;\\;\\;\\;\\;\\;\\;\\; (4)\n",
        "\\end{equation}\n",
        "para todo $l$, $ij$ y $m$, donde el parámetro $0<\\eta\\ll 1$ controla la tasa de aprendizaje.\n",
        "La iteración se detiene cuando ya no se advierten reducciones significativas del error $E$.\n",
        "\n",
        "## Cálculo del gradiente del error cuadrático\n",
        "\n",
        "Con el fin de simplificar la notación, elegimos un valor arbitrario de $m$ y obviamos la dependencia de las expresiones con éste índice.\n",
        "\n",
        "Notar que los vectores $x^l$ y $h^l$ sólo dependen de las matrices $w^1,...,w^{l}$.\n",
        "De esta manera, observamos que\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial x^l_i}{\\partial w^r_{pq}}\n",
        "&=&\n",
        "g'(h^l_i)\n",
        "\\frac{\\partial h^l_i}{\\partial w^r_{pq}}\n",
        "\\nonumber\n",
        "\\end{eqnarray}\n",
        "si $r\\leq l$, y\n",
        "$$\n",
        "\\frac{\\partial x^l_i}{\\partial w^r_{pq}}=0\n",
        "$$\n",
        "en caso contrario.\n",
        "Por otro lado,\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial h^{l}_i}{\\partial w^r_{pq}}\n",
        "&=&\n",
        "\\frac{\\partial}{\\partial w^r_{pq}}\n",
        "\\bigg(\n",
        "\\sum_j w^{l}_{ij} x^{l-1}_j\n",
        "\\bigg)\n",
        "\\nonumber\n",
        "\\\\\n",
        "&=&\n",
        "\\sum_j w^{l}_{ij}\n",
        "\\frac{\\partial x^{l-1}_j}{\\partial w^r_{pq}}\n",
        "\\nonumber\n",
        "\\end{eqnarray}\n",
        "si $r<l$, y\n",
        "$$\n",
        "\\frac{\\partial h^l_i}{\\partial w^{l}_{pq}}\n",
        "=\n",
        "\\sum_j\n",
        "\\delta_{ip}\n",
        "\\delta_{jq}\n",
        "x^{l-1}_j\n",
        "=\n",
        "\\delta_{ip}\n",
        "x^{l-1}_q\n",
        "$$\n",
        "Con estas ecuaciones se pueden establecer una relación de recurrencia que nos permite calcular las componentes del gradiente de $F$.\n",
        "A saber\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial F}{\\partial w^r_{pq}}\n",
        "&=&\n",
        "\\sum_i (x^L_i-s_i)\n",
        "\\frac{\\partial x^L}{\\partial w^r_{pq}}\n",
        "\\nonumber\n",
        "\\\\\n",
        "&=&\n",
        "\\sum_i (x^L_i-s_i)\n",
        "g'(h^L_i)\n",
        "\\frac{\\partial h^L_i}{\\partial w^r_{pq}}\n",
        "\\nonumber\n",
        "\\\\\n",
        "&=&\n",
        "\\sum_i\n",
        "D^L_i\n",
        "\\frac{\\partial h^L_i}{\\partial w^r_{pq}}\n",
        "\\nonumber\n",
        "\\\\\n",
        "&=&\n",
        "\\sum_i\n",
        "D^L_i\n",
        "\\sum_j\n",
        "w^L_{ij}\n",
        "\\frac{\\partial x^{L-1}_i}{\\partial w^r_{pq}}\n",
        "\\nonumber\n",
        "\\\\\n",
        "&=&\n",
        "\\sum_i\n",
        "D^L_i\n",
        "\\sum_j\n",
        "w^L_{ij}\n",
        "g'(h^{L-1}_j)\n",
        "\\frac{\\partial h^{L-1}_i}{\\partial w^r_{pq}}\n",
        "\\nonumber\n",
        "\\\\\n",
        "&=&\n",
        "\\sum_j\n",
        "\\bigg(\n",
        "g'(h^{L-1}_j)\n",
        "\\sum_i\n",
        "w^L_{ij}\n",
        "D^L_i\n",
        "\\bigg)\n",
        "\\frac{\\partial h^{L-1}_i}{\\partial w^r_{pq}}\n",
        "\\nonumber\n",
        "\\\\\n",
        "&=&\n",
        "\\sum_j\n",
        "D^{L-1}_j\n",
        "\\frac{\\partial h^{L-1}_i}{\\partial w^r_{pq}}\n",
        "\\nonumber\n",
        "\\end{eqnarray}\n",
        "donde\n",
        "\\begin{equation}\n",
        "D^L_i:=(x^L_i-s_i)g'(h^L_i)\n",
        "\\;\\;\\;\\;\\;\\;\\;\\; (5)\n",
        "\\end{equation}\n",
        "y\n",
        "$$\n",
        "D^{L-1}_j\n",
        ":=\n",
        "g'(h^{L-1}_j)\n",
        "\\sum_i\n",
        "w^L_{ij}\n",
        "D^L_i\n",
        "$$\n",
        "representan los *errores locales* de la $i$-ésima neurona en la $L$-ésima capa y la $j$-ésima neurona en la $(L-1)$-ésima capa, respectivamente.\n",
        "\n",
        "El anterior procedimiento puede continuarse capa por capa, con cada capa $l$ tal que $r<l$, de manera que\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial F}{\\partial w^r_{pq}}\n",
        "&=&\n",
        "\\sum_j D_j^l\n",
        "\\frac{\\partial h^l_j}{\\partial w^r_{pq}}\n",
        "\\nonumber\n",
        "\\end{eqnarray}\n",
        "donde\n",
        "\\begin{equation}\n",
        "D_j^l\n",
        ":=\n",
        "g'(h^{l}_j)\n",
        "\\sum_i w^{l+1}_{ij}D_i^{l+1}\n",
        "\\;\\;\\;\\;\\;\\;\\;\\; (6)\n",
        "\\end{equation}\n",
        "hasta que eventualmente se alcanza la capa $l=r$, y se obtiene\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial F}{\\partial w^r_{pq}}\n",
        "&=&\n",
        "\\sum_j\n",
        "D_j^{r}\n",
        "\\frac{\\partial h^{r}_j}{\\partial w^r_{pq}}\n",
        "\\nonumber\n",
        "\\\\\n",
        "&=&\n",
        "\\sum_j\n",
        "D_j^{r}\n",
        "\\delta_{jp}\n",
        "x^{r-1}_q\n",
        "\\nonumber\n",
        "\\\\\n",
        "&=&\n",
        "D_p^{r}\n",
        "x^{r-1}_q\n",
        "\\nonumber\n",
        "\\end{eqnarray}\n",
        "En particular, este último resultado se verifica para el caso $r=L$ de $pq$ arbitrario.\n",
        "También se verifica para el caso en que $q=n^{r-1}+1$ y valores arbitrarios de $r$ y $p$, en donde $x_q^{r-1}=-1$ corresponde al estado fijo de la neurona en la capa $(r-1)$-ésima que permite simular la acción de umbrales en la capa $r$-ésima, tal como se describe en la Ec. 3.\n",
        "\n",
        "## El algoritmo de backpropagation\n",
        "\n",
        "Los resultados anteriores pueden condensarse en el llamado *algoritmo de backpropagation*, el cuál permite el cálculo del gradiente y la actualización de los pesos sinápticos, y consiste en la siguiente lista de pasos.\n",
        "Para cada ejemplo $m=1,...,M$, ejecutar:\n",
        "1. *Forward pass:* calcular la salida $x^L$ de la red ante la entrada $x^1=e^m$ utilizando las Ecs. 1 y 2. En el proceso, guardar los valores de activación $x^l$ y de los correspondientes campos locales $h^l$ obtenidos en las distintas capas $l=2,...,L$, ya que serán útiles más adelante.\n",
        "2. Calcular el vector de errores $D^L$ de la capa de salida utilizando la Ec. 5.\n",
        "3. Propagar los errores hacia atrás, i.e. calcular los errores $D^l$ para $l=L-1,L-2,...,1$ utilizando la Ec. 6.\n",
        "4. Para cada $l$, $i$ y $j$, calcular el gradiente $\\frac{\\partial F^m}{\\partial w^l_{ij}}$ utilizando la Ec. 7 y actualizar el correspondiente peso sináptico $w^l_{ij}$ utilizando la Ec. 4."
      ],
      "metadata": {
        "id": "NRYEofSD0xoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1)"
      ],
      "metadata": {
        "id": "UjbcNI0a4ac3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
