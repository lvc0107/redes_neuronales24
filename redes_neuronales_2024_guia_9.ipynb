{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron monocapa\n",
        "\n",
        "Un perceptrón monocapa está compuesto de una capa pasiva de entrada, y una sola capa activa que también sirve de capa de salida.\n",
        "\n",
        "El input de un perceptron determina el estado de las neuronas pasivas de la capa de entrada, $x$.\n",
        "Se considera, además, una neurona pasiva de estado fijo $x_{n_e}=-1$, para que haga las veces de umbral de activación.\n",
        "Ante una entrada $x$, la salida de la red neuronal viene dada por\n",
        "$$\n",
        "y_j(x)\n",
        "=\n",
        "g(h_{j}(x))\n",
        "$$\n",
        "donde\n",
        "$$\n",
        "h_j\n",
        "=\n",
        "%\\sum_{i=1}^{n_e}\n",
        "\\sum_i\n",
        "w_{ji}x_i\n",
        "$$\n",
        "para $j=1,...,n_s$, y $g$ es una función de activación.\n",
        "Por ejemplo, una ReLU, la cual viene dada por\n",
        "$g(h) = h$ si $h>0$ y $g(h)=0$ si $h\\leq 0$.\n",
        "\n",
        "Para entrenar la red, usamos como función costo el error cuadrático sobre el conjunto de entrenamiento $\\{e_m,s_m:m=1,...,M\\}$, al cuál lo expresamos como una función de $w$\n",
        "$$\n",
        "E(w)\n",
        "=\n",
        "\\frac{1}{2}\n",
        "%\\sum_{m=1}^M\n",
        "\\sum_{m}\n",
        "%\\sum_{j=1}^{n_s}\n",
        "\\sum_{j}\n",
        "(y_{jm}(w)-s_{mj})^2\n",
        "$$\n",
        "donde $s_{mj}$ es la salida deseada en la $j$-ésima neurona ante el $m$-ésimo ejemplo, $y_{mj}$ es la salida obtenida en la $j$-esima neurona ante el $m$-ésimo ejemplo, y $n_s$ es el número de neuronas de salida.\n",
        "Por otro lado,\n",
        "$$\n",
        "y_{jm}(w)\n",
        "=\n",
        "g(h_{jm}(w))\n",
        "$$\n",
        "donde\n",
        "$$\n",
        "h_{jm}(w)\n",
        "=\n",
        "\\sum_{i=0}^{n_e}\n",
        "w_{ji}e_{mi}\n",
        "$$\n",
        "Nos interesa calcular el gradiente de $E(w)$\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial E}{\\partial w_{pq}}\n",
        "&=&\n",
        "%\\sum_{m=1}^M\n",
        "\\sum_m\n",
        "%\\sum_{j=1}^{n_s}\n",
        "\\sum_j\n",
        "(y_{jm}(w)-s_{mj})\n",
        "\\frac{\\partial y_{jm}}{\\partial w_{pq}}\n",
        "\\\\\n",
        "&=&\n",
        "%\\sum_{m=1}^M\n",
        "\\sum_m\n",
        "%\\sum_{j=1}^{n_s}\n",
        "\\sum_j\n",
        "(y_{jm}(w)-s_{mj})\n",
        "g'(h_{jm}(w))\n",
        "\\frac{\\partial h_{jm}}{\\partial w_{pq}}\n",
        "\\\\\n",
        "&=&\n",
        "%\\sum_{m=1}^M\n",
        "\\sum_m\n",
        "%\\sum_{j=1}^{n_s}\n",
        "\\sum_j\n",
        "(y_{jm}(w)-s_{mj})\n",
        "g'(h_{jm}(w))\n",
        "\\delta_{jp}\n",
        "e_{mq}\n",
        "\\\\\n",
        "&=&\n",
        "%\\sum_{m=1}^M\n",
        "\\sum_m\n",
        "(y_{pm}(w)-s_{mp})\n",
        "g'(h_{pm}(w))\n",
        "e_{mq}\n",
        "\\end{eqnarray}\n",
        "puesto que\n",
        "$$\n",
        "\\frac{\\partial h_{jm}}{\\partial w_{pq}}\n",
        "=\n",
        "%\\sum_{i=1}^{n_e}\n",
        "\\sum_i\n",
        "\\frac{w_{ji}}{w_{pq}}\n",
        "e_{mi}\n",
        "=\n",
        "%\\sum_{i=1}^{n_e}\n",
        "\\sum_i\n",
        "\\delta_{jp}\n",
        "\\delta_{iq}\n",
        "e_{mi}\n",
        "=\n",
        "\\delta_{jp}\n",
        "e_{mq}\n",
        "$$\n",
        "\n",
        "Recordar que, en el caso de una ReLU, $g'(h)=\\Theta(h)$, donde $\\Theta(h)=1$ si $h>0$ y $\\Theta(h)=0$ si $h\\leq 0$.\n",
        "\n",
        "Para actualizar los pesos sinápticos en la $(\\tau+1)$-ésima época de entrenamiento, utilice la regla\n",
        "$$\n",
        "w^{\\tau}_{ji} \\to w^{\\tau+1}_{ji} = w^{\\tau}_{ji} - \\eta \\frac{\\partial E}{\\partial w_{ji}}\n",
        "$$\n",
        "para todo $ji$.\n",
        "\n",
        "## **Ejercicio 1**\n",
        "\n",
        "Genere un conjunto de entrenamiento compuesto por $M$ puntos en $\\mathbb{R}^{n_e}$, distribuidos en $n_s$ nubes, con $m_c$ puntos en la nube $c$.\n",
        "Notar que $c=1,...,n_e$ nubes y, en total, se generarán $M=\\sum_c m_c$ puntos.\n",
        "\n",
        "Para generar las nubes:\n",
        "\n",
        "* genere aleatoriamente $n_s$ puntos en $\\mathbb{R}^{n_e}$ a los que llamaremos centros, sorteando los valores de las coordenadas a partir de una distribución normal, y\n",
        "\n",
        "* para cada centro $c$, genere $m_c$ puntos aleatorios alrededor del mismo, sumando sus coordenadas a números aleatorios generados con una Gaussiana de desviación estandard $\\sigma=0.1$.\n",
        "\n",
        "Las $n_e$ coordenadas del $m$-ésimo punto constituirán el vector de entrada del $m$-ésimo ejemplo.\n",
        "La nube a la que pertenece el $m$-ésimo punto determinará el vector de salida del $m$-ésimo ejemplo.\n",
        "Más precisamente, si el $m$-ésimo punto pertenence a la $c$-ésima nube, el vector de salida será el vector canónico $(0,0,...,1,...,0)$ de $n_s$ componentes con un único 1 en la $c$-esima posición.\n",
        "\n",
        "Concretamente\n",
        "\n",
        "1. Genere un conjunto de 8 puntos en $\\mathbb{R}^{n_e}$ con $n_e=2$, divididos en 3 nubes con $m_1=3$ en la primera nube, $m_2=2$ puntos en la segunda nube y $m_3=3$ puntos en la tercera nube. Utilice $\\sigma=0.1$ para indicar la dispersión de los puntos alrededor de cada nube.\n",
        "\n",
        "2. Grafique las nubes de puntos, utilizando un color distinto para cada una de ellas.\n",
        "\n",
        "**IMPORTANTE:** No olvide extender la entrada con una unidad extra de estado fijo $x_{n_e+1}=-1$ para que las sinapsis $w_{j,n_e+1}$ hagan las veces de umbrales $u_j$.\n",
        "\n",
        "## **Ejercicio 2**\n",
        "\n",
        "Implemente y entrene un **perceptrón monocapa** sobre el conjunto de entrenamiento generado en el Ejercicio 1.\n",
        "Utilice funciones de activación **sigmoideas** y, además, recuerde agregar las neuronas auxiliares que permiten imitar los umbrales de activación.\n",
        "\n",
        "Para entrenarlo, utilice una taza $\\eta=0.02$ y alrededor de 500.000 de épocas o más, según considere necesario.\n",
        "\n",
        "Luego, grafique nuevamente los puntos, pintando el relleno de los mismos con los colores de las nubes asociadas, y el borde de los mismos con el color correspondiente a la predicción.\n",
        "Grafique, además, las predicciones antes de entrar con el fin de corroborar que la red sin entregar clasifica erroneamente los ejemplos.\n",
        "\n",
        "## **Ejercicio 3**\n",
        "\n",
        "La compuerta XOR.\n",
        "\n",
        "El siguiente conjunto de 4 ejemplos:\n",
        "\n",
        "* $e_1 = (0,0,-1)$, $s_1=(1,0)$\n",
        "* $e_2 = (0,1,-1)$, $s_2=(0,1)$\n",
        "* $e_3 = (1,0,-1)$, $s_3=(0,1)$\n",
        "* $e_4 = (1,1,-1)$, $s_4=(1,0)$\n",
        "\n",
        "corresponde a la compuerta XOR.\n",
        "Utilice el **perceptrón monocapa** implementando para verificar que el mismo no es capáz de aprender este conjunto de ejemplos.\n",
        "\n",
        "## **Ejercicio 4**\n",
        "\n",
        "Repita los experimentos utilizando funciones de activación de tipo **ReLU**."
      ],
      "metadata": {
        "id": "NRYEofSD0xoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1)"
      ],
      "metadata": {
        "id": "UjbcNI0a4ac3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}